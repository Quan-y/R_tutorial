# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K = 5, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
realResult <- test$Direction
# test error
sum(!modelResult==realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008, ]
test <- Weekly[Weekly$Year > 2008, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K = 5, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
realResult <- test$Direction
# test error
sum(!modelResult==realResult)/nrow(test)
set.seed(1) # Please don't remove this code!
# install.packages("ISLR")
library(ISLR)
## Scatterplot Lag 1
plot(Weekly$Lag1,Weekly$Today,main="Returns",xlab="Two Weeks Ago",ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag1),col="red")
## Boxplot Lag 1
boxplot(Weekly$Lag1~Weekly$Direction,main="Returns",ylab="One Week Ago",xlab="Direction")
## Scatterplot Lag 1
plot(Weekly$Lag5,Weekly$Today,main="Returns",xlab="Five Weeks Ago",ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag5),col="red")
## Boxplot Lag 5
boxplot(Weekly$Lag5~Weekly$Direction,main="Returns",ylab="Five Week Ago",xlab="Direction")
KNN.decision <- function(Lag1.new, Lag2.new, K = 5, Lag1 = Smarket$Lag1, Lag2 = Smarket$Lag2) {
n <- length(Lag1)
stopifnot(length(Lag2) == n, length(Lag1.new) == 1, length(Lag2.new) == 1, K <= n)
dists       <- sqrt((Lag1-Lag1.new)^2 + (Lag2-Lag2.new)^2)
neighbors  <- order(dists)[1:K]
neighb.dir <- Smarket$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
par(mfrow=c(2,3))
## Scatterplot Lag 1
plot(Weekly$Lag1, Weekly$Today, main="Returns", xlab="One Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag1),col="red")
## Scatterplot Lag 2
plot(Weekly$Lag2, Weekly$Today, main="Returns", xlab="Two Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag2),col="red")
## Scatterplot Lag 3
plot(Weekly$Lag3, Weekly$Today, main="Returns", xlab="Three Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag3),col="red")
## Scatterplot Lag 4
plot(Weekly$Lag4, Weekly$Today, main="Returns", xlab="Four Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag4),col="red")
## Scatterplot Lag 5
plot(Weekly$Lag5, Weekly$Today, main="Returns", xlab="Five Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag5),col="red")
## Boxplot Lag 5
boxplot(Weekly$Lag1~Weekly$Direction,main="Returns",ylab="One Week Ago",xlab="Direction")
## Scatterplot Lag 1
plot(Weekly$Lag1,Weekly$Today,main="Returns",xlab="Two Weeks Ago",ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag1),col="red")
## Boxplot Lag 1
boxplot(Weekly$Lag1~Weekly$Direction,main="Returns",ylab="One Week Ago",xlab="Direction")
KNN.decision <- function(newpoint, K = 5, Lag = Weekly[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- Weekly$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
KNN.decision(newpoint = c(0.1,0.1,0.1,0.1,0.1), Lag = Weekly[,2:6])
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008, ]
test <- Weekly[Weekly$Year > 2008, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
realResult <- test$Direction
# test error
sum(!modelResult==realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
modelResult2
modelResult
sum(!modelResult2 == realResult)
sum(!modelResult == realResult)
modelResult2 == modelResult
sum(modelResult2 == modelResult)
sum(modelResult2 == realResult)
sum(modelResult == realResult)
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008, ]
test <- Weekly[Weekly$Year > 2008, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
realResult <- test$Direction
# test error
sum(!modelResult == realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
set.seed(1) # Please don't remove this code!
# install.packages("ISLR")
library(ISLR)
## Scatterplot Lag 1
plot(Weekly$Lag1,Weekly$Today,main="Returns",xlab="Two Weeks Ago",ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag1),col="red")
## Boxplot Lag 1
boxplot(Weekly$Lag1~Weekly$Direction,main="Returns",ylab="One Week Ago",xlab="Direction")
par(mfrow=c(2,3))
## Scatterplot Lag 1
plot(Weekly$Lag1, Weekly$Today, main="Returns", xlab="One Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag1),col="red")
## Scatterplot Lag 2
plot(Weekly$Lag2, Weekly$Today, main="Returns", xlab="Two Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag2),col="red")
## Scatterplot Lag 3
plot(Weekly$Lag3, Weekly$Today, main="Returns", xlab="Three Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag3),col="red")
## Scatterplot Lag 4
plot(Weekly$Lag4, Weekly$Today, main="Returns", xlab="Four Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag4),col="red")
## Scatterplot Lag 5
plot(Weekly$Lag5, Weekly$Today, main="Returns", xlab="Five Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag5),col="red")
## Boxplot Lag 5
boxplot(Weekly$Lag1~Weekly$Direction,main="Returns",ylab="One Week Ago",xlab="Direction")
KNN.decision <- function(Lag1.new, Lag2.new, K = 5, Lag1 = Smarket$Lag1, Lag2 = Smarket$Lag2) {
n <- length(Lag1)
stopifnot(length(Lag2) == n, length(Lag1.new) == 1, length(Lag2.new) == 1, K <= n)
dists       <- sqrt((Lag1-Lag1.new)^2 + (Lag2-Lag2.new)^2)
neighbors  <- order(dists)[1:K]
neighb.dir <- Smarket$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
KNN.decision <- function(newpoint, K = 5, Lag = Weekly[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- Weekly$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
KNN.decision(newpoint = c(0.1,0.1,0.1,0.1,0.1), Lag = Weekly[,2:6])
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990]
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990]
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990, ]
test <- Weekly[Weekly$Year >= 2009 & Weekly$Year <= 2010, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
realResult <- test$Direction
# test error
sum(!modelResult == realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 2, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 4, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990, ]
test <- Weekly[Weekly$Year >= 2009 & Weekly$Year <= 2010, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
realResult <- test$Direction
# test error
sum(!modelResult == realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990, ]
test <- Weekly[Weekly$Year >= 2009 & Weekly$Year <= 2010, ]
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990, ]
test <- Weekly[Weekly$Year >= 2009 & Weekly$Year <= 2010, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
dists2 <- colSums((apply(Lag,1,"-",Lag.new))^2)
cat(dists == dists2)
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990, ]
test <- Weekly[Weekly$Year >= 2009 & Weekly$Year <= 2010, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
dists2 <- colSums((apply(Lag,1,"-",newpoint))^2)
cat(dists == dists2)
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990, ]
test <- Weekly[Weekly$Year >= 2009 & Weekly$Year <= 2010, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
dists2 <- colSums((apply(Lag,1,"-",newpoint))^2)
cat(dists2)
# cat(dists)
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
dim(train[,2:6])
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990, ]
test <- Weekly[Weekly$Year >= 2009 & Weekly$Year <= 2010, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# cat(dists)
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
realResult <- test$Direction
# test error
sum(!modelResult == realResult)/nrow(test)
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
realResult <- test$Direction
# test error
sum(!modelResult == realResult)/nrow(test)
test$Direction
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
KNN.decision2 <- function(Lag.new, K = 5, Lag)
{
n <- nrow(Lag)
stopifnot(ncol(Lag) == 5, length(Lag.new) == 5, K <= n)
dists <- colSums((apply(Lag,1,"-",Lag.new))^2)
neighbors <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice <- names(which.max(table(neighb.dir)))
return(choice)
}
KNN.decision2(Lag.new = unlist(test[0,2:6]), K=5, Lag = train[,2:6])
