set.seed(1) # Please don't remove this code!
# install.packages("ISLR")
library(ISLR)
## Scatterplot Lag 1
plot(Weekly$Lag1,Weekly$Today,main="Returns",xlab="Two Weeks Ago",ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag1),col="red")
## Boxplot Lag 1
boxplot(Weekly$Lag1~Weekly$Direction,main="Returns",ylab="One Week Ago",xlab="Direction")
par(mfrow=c(2,3))
## Scatterplot Lag 1
plot(Weekly$Lag1, Weekly$Today, main="Returns", xlab="One Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag1),col="red")
## Scatterplot Lag 2
plot(Weekly$Lag2, Weekly$Today, main="Returns", xlab="Two Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag2),col="red")
## Scatterplot Lag 3
plot(Weekly$Lag3, Weekly$Today, main="Returns", xlab="Three Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag3),col="red")
## Scatterplot Lag 4
plot(Weekly$Lag4, Weekly$Today, main="Returns", xlab="Four Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag4),col="red")
## Scatterplot Lag 5
plot(Weekly$Lag5, Weekly$Today, main="Returns", xlab="Five Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag5),col="red")
## Boxplot Lag 5
boxplot(Weekly$Lag1~Weekly$Direction,main="Returns",ylab="One Week Ago",xlab="Direction")
KNN.decision <- function(Lag1.new, Lag2.new, K = 5, Lag1 = Smarket$Lag1, Lag2 = Smarket$Lag2) {
n <- length(Lag1)
stopifnot(length(Lag2) == n, length(Lag1.new) == 1, length(Lag2.new) == 1, K <= n)
dists       <- sqrt((Lag1-Lag1.new)^2 + (Lag2-Lag2.new)^2)
neighbors  <- order(dists)[1:K]
neighb.dir <- Smarket$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
KNN.decision <- function(newpoint, K = 5, Lag = Weekly[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- Weekly$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
KNN.decision(newpoint = c(0.1,0.1,0.1,0.1,0.1), Lag = Weekly[,2:6])
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990, ]
test <- Weekly[Weekly$Year >= 2009 & Weekly$Year <= 2010, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# cat(dists)
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
realResult <- test$Direction
# test error
sum(!modelResult == realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
train <- Weekly[Weekly$Year>=1990 & Weekly$Year<=2008,]
test <- Weekly[Weekly$Year>=2009 & Weekly$Year<=2010,]
n.test <- nrow(test)
test$Predict <- NA
for(i in 1:n.test)
{
test$Predict[i] <- KNN.decision(Lag.new=unlist(test[i,2:6]),K=5,Lag=train[,2:6])
}
KNN.decision <- function(Lag.new, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# cat(dists)
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
train <- Weekly[Weekly$Year>=1990 & Weekly$Year<=2008,]
test <- Weekly[Weekly$Year>=2009 & Weekly$Year<=2010,]
n.test <- nrow(test)
test$Predict <- NA
for(i in 1:n.test)
{
test$Predict[i] <- KNN.decision(Lag.new=unlist(test[i,2:6]),K=5,Lag=train[,2:6])
}
KNN.decision <- function(Lag.new, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(Lag.new) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - Lag.new)**2))
# cat(dists)
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
train <- Weekly[Weekly$Year>=1990 & Weekly$Year<=2008,]
test <- Weekly[Weekly$Year>=2009 & Weekly$Year<=2010,]
n.test <- nrow(test)
test$Predict <- NA
for(i in 1:n.test)
{
test$Predict[i] <- KNN.decision(Lag.new=unlist(test[i,2:6]),K=5,Lag=train[,2:6])
}
test.error <- sum(test$Direction!=test$Predict)/nrow(test)
test.error
train <- Weekly[Weekly$Year>=1990 & Weekly$Year<=2008,]
test <- Weekly[Weekly$Year>=2009 & Weekly$Year<=2010,]
n.test <- nrow(test)
test$Predict <- NA
for(i in 1:n.test)
{
test$Predict[i] <- KNN.decision(Lag.new=unlist(test[i,2:6]),K=3,Lag=train[,2:6])
}
test.error <- sum(test$Direction!=test$Predict)/nrow(test)
test.error
train[,2:6] - test[1,2:6]
test[1,2:6]
head(train[,2:6], 5)
test[[1,2:6]]
c(test[1,2:6])
test[1,2:6]
test[1,2:6][986]
test[1
test[1]
test[1]
test[,1]
test[1,]
test[1,2:6][1,]
unname(test[1,2:6][1,])
as.vector(test[1,2:6][1,])
set.seed(1) # Please don't remove this code!
# install.packages("ISLR")
library(ISLR)
## Scatterplot Lag 1
plot(Weekly$Lag1,Weekly$Today,main="Returns",xlab="Two Weeks Ago",ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag1),col="red")
## Boxplot Lag 1
boxplot(Weekly$Lag1~Weekly$Direction,main="Returns",ylab="One Week Ago",xlab="Direction")
par(mfrow=c(2,3))
## Scatterplot Lag 1
plot(Weekly$Lag1, Weekly$Today, main="Returns", xlab="One Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag1),col="red")
## Scatterplot Lag 2
plot(Weekly$Lag2, Weekly$Today, main="Returns", xlab="Two Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag2),col="red")
## Scatterplot Lag 3
plot(Weekly$Lag3, Weekly$Today, main="Returns", xlab="Three Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag3),col="red")
## Scatterplot Lag 4
plot(Weekly$Lag4, Weekly$Today, main="Returns", xlab="Four Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag4),col="red")
## Scatterplot Lag 5
plot(Weekly$Lag5, Weekly$Today, main="Returns", xlab="Five Weeks Ago", ylab="Today")
abline(lm(Weekly$Today~Weekly$Lag5),col="red")
## Boxplot Lag 5
boxplot(Weekly$Lag1~Weekly$Direction,main="Returns",ylab="One Week Ago",xlab="Direction")
KNN.decision <- function(Lag1.new, Lag2.new, K = 5, Lag1 = Smarket$Lag1, Lag2 = Smarket$Lag2) {
n <- length(Lag1)
stopifnot(length(Lag2) == n, length(Lag1.new) == 1, length(Lag2.new) == 1, K <= n)
dists       <- sqrt((Lag1-Lag1.new)^2 + (Lag2-Lag2.new)^2)
neighbors  <- order(dists)[1:K]
neighb.dir <- Smarket$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
KNN.decision <- function(newpoint, K = 5, Lag = Weekly[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- sqrt(rowSums((Lag - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- Weekly$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
KNN.decision(newpoint = c(0.1,0.1,0.1,0.1,0.1), Lag = Weekly[,2:6])
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990, ]
test <- Weekly[Weekly$Year >= 2009 & Weekly$Year <= 2010, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- colSums((apply(Lag, 1, "-",Lag.new))^2)
# cat(dists)
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
# Notice that we random select train dataset 80%, test dataset 20%
# if we just want to use the train dataset from 1990-2008, test dataset from 2009-2010
# just run the following code and comment the following prepare data part
train <- Weekly[Weekly$Year <= 2008 & Weekly$Year >= 1990, ]
test <- Weekly[Weekly$Year >= 2009 & Weekly$Year <= 2010, ]
# --------------------------- prepare data --------------------------- #
# # get train size
# trainSize <- floor(nrow(Weekly)*0.8)
# # get train id
# train_ind <- sample(seq_len(nrow(Weekly)), size = trainSize, replace = FALSE, prob = NULL)
# # train id
# train <- Weekly[train_ind, ]
# # test id
# test <- Weekly[-train_ind, ]
# ------------------------- KNN classification ------------------------ #
# train
KNN.decision <- function(newpoint, K, Lag = train[,2:6]) {
n <- nrow(Lag)
# check valid input
stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- colSums((apply(Lag, 1, "-", newpoint))^2)
# cat(dists)
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- train$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
# test
modelResult <- apply(test[,2:6], 1, KNN.decision, K = 5, Lag = train[,2:6])
realResult <- test$Direction
# test error
sum(!modelResult == realResult)/nrow(test)
# test
modelResult2 <- apply(test[,2:6], 1, KNN.decision, K = 3, Lag = train[,2:6])
# test error
sum(!modelResult2 == realResult)/nrow(test)
# create vector
flodVec <- rep(1:9,121)
# random order
rand.flodVec <- sample(flodVec, size = nrow(Weekly), replace = FALSE)
rand.flodVec
# create vector
flodVec <- rep(1:9,121)
# random order
rand.flodVec <- sample(flodVec, size = nrow(Weekly), replace = FALSE)
new.KNN.decision <- function(newpoint, itrain, K = 5) {
# n <- nrow(train)
# # check valid input
# stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- colSums((apply(itrain[,1:5], 1, "-", newpoint))^2)
# dists <- sqrt(rowSums((itrain[,1:5] - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- etrain$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
test.score <- function(etrain, etest, model = new.KNN.decision, para = 5){
# test
modelResult <- apply(etest[,1:5], 1, model, para, itrain = etrain)
realResult <- test$Direction
# test error
return(sum(!modelResult==realResult)/nrow(etest))
}
new.Weekly <- Weekly[,c(2:6, 9)]
new.Weekly$fold = rand.flodVec
# k-fold test
ERR <- c()
for (i in 1:9){
etrain <- new.Weekly[new.Weekly$fold != i, 1:6]
etest <- new.Weekly[new.Weekly$fold == i, 1:6]
ERR <- c(ERR, test.score(etrain, etest))
}
# average error
cat('average test error', mean(ERR))
new.KNN.decision <- function(newpoint, itrain, K = 5) {
# n <- nrow(train)
# # check valid input
# stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- colSums((apply(itrain[,1:5], 1, "-", newpoint))^2)
# dists <- sqrt(rowSums((itrain[,1:5] - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- etrain$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
test.score <- function(etrain, etest, model = new.KNN.decision, para = 5){
# test
modelResult <- apply(etest[,1:5], 1, model, para, itrain = etrain)
realResult <- test$Direction
# test error
return(sum(!modelResult==realResult)/nrow(etest))
}
new.Weekly <- Weekly[,c(2:6, 9)]
new.Weekly$fold = rand.flodVec
# k-fold test
ERR <- c()
for (i in 1:9){
etrain <- new.Weekly[new.Weekly$fold != i, 1:6]
etest <- new.Weekly[new.Weekly$fold == i, 1:6]
ERR <- c(ERR, test.score(etrain, etest))
}
new.KNN.decision <- function(newpoint, itrain, K = 5) {
# n <- nrow(train)
# # check valid input
# stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- colSums((apply(itrain[,1:5], 1, "-", newpoint))^2)
# dists <- sqrt(rowSums((itrain[,1:5] - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- etrain$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
test.score <- function(etrain, etest, model = new.KNN.decision, para = 5){
# test
modelResult <- apply(etest[,1:5], 1, model, para, itrain = etrain)
realResult <- test$Direction
# test error
return(sum(!modelResult==realResult)/nrow(etest))
}
new.Weekly <- Weekly[,c(2:6, 9)]
new.Weekly$fold = rand.flodVec
# k-fold test
total.ERR <- 0
for (i in 1:9){
etrain <- new.Weekly[new.Weekly$fold != i, 1:6]
etest <- new.Weekly[new.Weekly$fold == i, 1:6]
total.ERR = total.ERR + test.score(etrain, etest)
}
# average error
cat('average test error', total.ERR/9)
test.score(etrain, etest)
test.score <- function(etrain, etest, model = new.KNN.decision, para = 5){
# test
modelResult <- apply(etest[,1:5], 1, model, para, itrain = etrain)
realResult <- test$Direction
# test error
return((nrow(etest) - sum(modelResult==realResult))/nrow(etest))
}
test.score(etrain, etest)
test.score <- function(etrain, etest, model = new.KNN.decision, para = 5){
# test
modelResult <- apply(etest[,1:5], 1, model, para, itrain = etrain)
realResult <- test$Direction
cat(length(modelResult), length(realResult))
# test error
return((nrow(etest) - sum(modelResult==realResult))/nrow(etest))
}
test.score(etrain, etest)
test.score <- function(etrain, etest, model = new.KNN.decision, para = 5){
# test
modelResult <- apply(etest[,1:5], 1, model, para, itrain = etrain)
realResult <- etest$Direction
cat(length(modelResult), length(realResult))
# test error
return((nrow(etest) - sum(modelResult==realResult))/nrow(etest))
}
new.KNN.decision <- function(newpoint, itrain, K = 5) {
# n <- nrow(train)
# # check valid input
# stopifnot(length(newpoint) == 5, K <= n)
# distance
dists <- colSums((apply(itrain[,1:5], 1, "-", newpoint))^2)
# dists <- sqrt(rowSums((itrain[,1:5] - newpoint)**2))
# order distance
neighbors  <- order(dists)[1:K]
neighb.dir <- etrain$Direction[neighbors]
choice      <- names(which.max(table(neighb.dir)))
return(choice)
}
test.score <- function(etrain, etest, model = new.KNN.decision, para = 5){
# test
modelResult <- apply(etest[,1:5], 1, model, para, itrain = etrain)
realResult <- etest$Direction
# test error
return((nrow(etest) - sum(modelResult==realResult))/nrow(etest))
}
new.Weekly <- Weekly[,c(2:6, 9)]
new.Weekly$fold = rand.flodVec
# k-fold test
total.ERR <- 0
for (i in 1:9){
etrain <- new.Weekly[new.Weekly$fold != i, 1:6]
etest <- new.Weekly[new.Weekly$fold == i, 1:6]
total.ERR = total.ERR + test.score(etrain, etest)
}
# average error
cat('average test error', total.ERR/9)
# ----------------------------- K = 1 ----------------------------- #
ERR <- c()
for (i in 1:9){
etrain <- new.Weekly[new.Weekly$fold != i, 1:6]
etest <- new.Weekly[new.Weekly$fold == i, 1:6]
ERR <- c(ERR, test.score(etrain, etest, model = new.KNN.decision, para = 1))
}
# average error
cat('average test error when K = 1', mean(ERR))
# ----------------------------- K = 3 ----------------------------- #
ERR <- c()
for (i in 1:9){
etrain <- new.Weekly[new.Weekly$fold != i, 1:6]
etest <- new.Weekly[new.Weekly$fold == i, 1:6]
ERR <- c(ERR, test.score(etrain, etest, model = new.KNN.decision, para = 3))
}
# average error
cat('average test error when K = 3', mean(ERR))
# ----------------------------- K = 7 ----------------------------- #
ERR <- c()
for (i in 1:9){
etrain <- new.Weekly[new.Weekly$fold != i, 1:6]
etest <- new.Weekly[new.Weekly$fold == i, 1:6]
ERR <- c(ERR, test.score(etrain, etest, model = new.KNN.decision, para = 7))
}
# average error
cat('average test error when K = 7', mean(ERR))
